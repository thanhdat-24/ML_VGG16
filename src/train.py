import tensorflow as tf
from keras.optimizers import Adam
from keras.callbacks import EarlyStopping, ReduceLROnPlateau
from model import build_vgg16, fine_tune_model
from data_preprocessing import get_data_generators
import config

# Load dá»¯ liá»‡u
train_generator, val_generator, _ = get_data_generators()

# XÃ¢y dá»±ng mÃ´ hÃ¬nh
model, base_model = build_vgg16()

# Compile mÃ´ hÃ¬nh ban Ä‘áº§u
model.compile(loss="categorical_crossentropy", optimizer=Adam(learning_rate=config.LEARNING_RATE), metrics=["accuracy"])

# Callback trÃ¡nh overfitting
callbacks = [
    EarlyStopping(monitor="val_loss", patience=5, restore_best_weights=True),
    ReduceLROnPlateau(monitor="val_loss", factor=0.1, patience=3, verbose=1)
]

# Huáº¥n luyá»‡n láº§n 1 (Ä‘Ã³ng bÄƒng base model)
print("ðŸš€ Huáº¥n luyá»‡n giai Ä‘oáº¡n 1 (base model frozen)...")
model.fit(train_generator, validation_data=val_generator, epochs=config.EPOCHS, callbacks=callbacks)

# Fine-tune
print("\nðŸ”¥ Báº¯t Ä‘áº§u fine-tuning vá»›i 4 layer cuá»‘i cá»§a VGG16 má»Ÿ khÃ³a...")
model = fine_tune_model(model, base_model, num_layers=4, lr=1e-5)

# Huáº¥n luyá»‡n láº§n 2 (fine-tune vá»›i learning rate tháº¥p)
model.fit(train_generator, validation_data=val_generator, epochs=config.FINE_TUNE_EPOCHS, callbacks=callbacks)

# LÆ°u mÃ´ hÃ¬nh
model.save(config.MODEL_SAVE_PATH)
print(f"âœ… MÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c lÆ°u táº¡i {config.MODEL_SAVE_PATH}")